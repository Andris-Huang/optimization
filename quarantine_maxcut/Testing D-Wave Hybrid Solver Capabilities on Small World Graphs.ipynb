{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from dwave.system.samplers import DWaveSampler\n",
    "from dwave.system import LeapHybridSampler\n",
    "from dwave.system.composites import EmbeddingComposite\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import networkx.generators.community as community\n",
    "from networkx.generators.random_graphs import connected_watts_strogatz_graph as small_world\n",
    "from networkx.generators.random_graphs import powerlaw_cluster_graph as cluster\n",
    "import ndlib\n",
    "import future.utils\n",
    "import warnings\n",
    "import time\n",
    "from random import randint\n",
    "from random import sample\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QUBO dict for D-Wave to optimize\n",
    "def maxcut_qubo(G):\n",
    "    # The Hamiltonian is H = sum(Pauli_Z(i) x Pauli_Z(j) - I)/2\n",
    "    # The eigenvalue is the negation of the edges in cut solution\n",
    "    # The QUBO should be -xi-xj+2xixj, we label it as Q\n",
    "    Q = defaultdict(int)\n",
    "    for u, v in G.edges:\n",
    "        Q[(u,u)]+= -1\n",
    "        Q[(v,v)]+= -1\n",
    "        Q[(u,v)]+= 2\n",
    "    return Q\n",
    "\n",
    "# Invoke D-Wave annealer\n",
    "def solve(Q, chainstrength=8, numruns=100):\n",
    "    # Run the QUBO on the solver from your config file\n",
    "    sampler = LeapHybridSampler()\n",
    "    response = sampler.sample_qubo(Q, num_reads=numruns)\n",
    "    energies = iter(response.data())\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_avg_distance(G, samples):\n",
    "    distance_sum = 0\n",
    "    for i in range(samples):\n",
    "        pair = sample(list(G.nodes()), 2)\n",
    "        #print(pair)\n",
    "        distance = len(nx.shortest_path(G, pair[0], pair[1]))\n",
    "        #print(distance)\n",
    "        distance_sum += distance\n",
    "    return distance_sum/samples\n",
    "\n",
    "def random_avg_clustering_coeff(G, samples):\n",
    "    sampled_nodes = sample(list(G.nodes()), samples)\n",
    "    clustering_dict = nx.clustering(G, sampled_nodes)\n",
    "    return np.mean(np.array(list(clustering_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04040404040404041, 0.04020100502512563, 0.040100250626566414, 0.04005006257822278, 0.04002501563477173, 0.04001250390747108, 0.04001250390747108, 0.04001250390747108]\n",
      "[0.4026666666666667, 0.48336507936507933, 0.4509361809501128, 0.4559357583175395, 0.46936473710331195, 0.452351081021374, 0.46249191141547513, 0.45652763727131296]\n",
      "[6.7, 4.52, 3.96, 3.64, 3.32, 3.1, 3.08, 3.24]\n"
     ]
    }
   ],
   "source": [
    "densities, cluster_coeffs, avg_distances = [], [], []\n",
    "a = small_world(100, 4, .05)\n",
    "densities.append(nx.density(a))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(a, 50))\n",
    "avg_distances.append(random_avg_distance(a, 50))\n",
    "b = small_world(200, 8, .1)\n",
    "densities.append(nx.density(b))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(b, 50))\n",
    "avg_distances.append(random_avg_distance(b, 50))\n",
    "c = small_world(400, 16, .14)\n",
    "densities.append(nx.density(c))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(c, 50))\n",
    "avg_distances.append(random_avg_distance(c, 50))\n",
    "d = small_world(800, 32, .15)\n",
    "densities.append(nx.density(d))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(d, 50))\n",
    "avg_distances.append(random_avg_distance(d, 50))\n",
    "e = small_world(1600, 64, .15)\n",
    "densities.append(nx.density(e))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(e, 50))\n",
    "avg_distances.append(random_avg_distance(e, 50))\n",
    "f = small_world(3200, 128, .15)\n",
    "densities.append(nx.density(f))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(f, 50))\n",
    "avg_distances.append(random_avg_distance(f, 50))\n",
    "g = small_world(6400, 256, .15)\n",
    "densities.append(nx.density(f))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(f, 50))\n",
    "avg_distances.append(random_avg_distance(f, 50))\n",
    "h = small_world(12800, 512, .15)\n",
    "densities.append(nx.density(f))\n",
    "cluster_coeffs.append(random_avg_clustering_coeff(f, 50))\n",
    "avg_distances.append(random_avg_distance(f, 50))\n",
    "print(densities) \n",
    "print(cluster_coeffs) \n",
    "print(avg_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed size 100\n",
      "completed size 200\n",
      "completed size 400\n",
      "completed size 800\n",
      "completed size 1600\n",
      "completed size 3200\n",
      "completed size 6400\n",
      "completed size 12800\n",
      "[1.7757821083068848, 1.9077961444854736, 1.6752736568450928, 1.9922738075256348, 2.7618930339813232, 6.263381481170654, 16.094152212142944, 90.20424818992615]\n"
     ]
    }
   ],
   "source": [
    "small_world_graphs = [a, b, c, d, e, f, g, h]\n",
    "small_world_times = []\n",
    "small_world_start_time = time.time()\n",
    "small_world_previous_time = small_world_start_time\n",
    "for graph, size in zip(small_world_graphs, [100, 200, 400, 800, 1600, 3200, 6400, 12800]):\n",
    "    result = solve(maxcut_qubo(graph))\n",
    "    small_world_current_time = time.time()\n",
    "    small_world_times.append(small_world_current_time - small_world_previous_time)\n",
    "    small_world_previous_time = small_world_current_time\n",
    "    print(f'completed size {size}')\n",
    "print(small_world_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hybrid solvers, we can solve Max-cut on a sparse graph of 12 thousand nodes in 90 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contributors: James Sud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwaveCorona",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
